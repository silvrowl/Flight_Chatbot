{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install spacy -qq\n",
    "\n",
    "#POC model that is able to extract locations (to and from), dates and prices ranges from conversational text.\n",
    "\n",
    "#Imports\n",
    "\n",
    "import nltk, nltk.tag, nltk.chunk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pprint as pprint\n",
    "from gensim.summarization import summarize\n",
    "from collections import Counter\n",
    "\n",
    "#import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "import re\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If I want to use a prepackaged NLP\n",
    "\n",
    "#ny_bb = 'Tlg bawa kami graduan dan family pulang.. They came for our convo but then cancelled unfortunately our flight has been cancelled too because egypt had suspended all international flight in and out. Most of airlines may not be operating till june'\n",
    "#article = nlp(ny_bb)\n",
    "#labels = [x.label_ for x in article.ents]\n",
    "#Counter(labels)\n",
    "#sentences = [x for x in article.sents]\n",
    "#displacy.render(nlp(str(sentences[:])), jupyter=True, style='ent')\n",
    "#print(sentences[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build My own NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to /home/dan/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Example Chats....\n",
    "nltk.download('nps_chat')\n",
    "from nltk.corpus import nps_chat\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "#for t in chatroom:\n",
    "#    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_Text_1 = [['Hi there'],\n",
    "               ['How can I help you?'],\n",
    "               ['Are there any flights from Houston to Boston'],\n",
    "               ['Yes, for which dates would you like?'],\n",
    "               ['June 3rd to June 18th'],\n",
    "               ['How much would like to spend?'],\n",
    "               ['Under 500'],\n",
    "               ['Great, Here are some options:']]\n",
    "\n",
    "\n",
    "Sample_Text_2 = [['Good Morning'],\n",
    "               ['How can i help you?'],\n",
    "               ['I would like to see if I can travel from Seattle to Chicago tomorrow'],\n",
    "               ['When would you like to come back'],\n",
    "               ['In 5 days, but I am flexible on the day'],\n",
    "               ['What is your budget'],\n",
    "               ['Less than fifteen hundred dollars'],\n",
    "               ['Great, Here are some options:']]\n",
    "               \n",
    "Sample_Text_3 = [['Yo'],\n",
    "               ['How can I help you?'],\n",
    "               ['How much is it to go from San Diego to Los Angeles. I have to get there for a wedding.'],\n",
    "               ['When would you like to travel'],\n",
    "               ['March 1st to March 2nd'],\n",
    "               ['What is your budget'],\n",
    "               ['The cheapest you can find'],\n",
    "               ['Great, Here are some options:']]\n",
    "\n",
    "Sample_Text_4 = [['Guten Tag'],\n",
    "               ['How can I help you?'],\n",
    "               ['I would like to fly to San Francisco on the 5th of December'],\n",
    "               ['What is your budget'],\n",
    "               ['Price is no object, and I would like first class if possible'],\n",
    "               ['Great, Here are some options:']]\n",
    "\n",
    "Sample_Text_5 = [['Hello'],\n",
    "               ['How can I help you?'],\n",
    "               ['I would like to fly to Portland as soon as possible'],\n",
    "               ['Where are you located'],\n",
    "               ['Washington'],\n",
    "               ['What is your budget'],\n",
    "               ['Between $400 and $500'],\n",
    "               ['Great, Here are some options:']]\n",
    "\n",
    "\n",
    "Complete_Sample = Sample_Text_1 + Sample_Text_2 + Sample_Text_3 + Sample_Text_4 + Sample_Text_5\n",
    "\n",
    "Complete_Sample = Sample_Text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- Replace written numbers with values\n",
    "- Replace ASAP with today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(x):\n",
    "    if type(x) == str:\n",
    "        x = x.replace(',', '')\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text2int (textnum, numwords={}):\n",
    "    units = [\n",
    "        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight',\n",
    "        'nine', 'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen',\n",
    "        'sixteen', 'seventeen', 'eighteen', 'nineteen',\n",
    "    ]\n",
    "    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
    "    scales = ['hundred', 'thousand', 'million', 'billion', 'trillion']\n",
    "    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}\n",
    "    ordinal_endings = [('ieth', 'y'), ('th', '')]\n",
    "\n",
    "    if not numwords:\n",
    "        numwords['and'] = (1, 0)\n",
    "        for idx, word in enumerate(units): numwords[word] = (1, idx)\n",
    "        for idx, word in enumerate(tens): numwords[word] = (1, idx * 10)\n",
    "        for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "    textnum = textnum.replace('-', ' ')\n",
    "\n",
    "    current = result = 0\n",
    "    curstring = ''\n",
    "    onnumber = False\n",
    "    lastunit = False\n",
    "    lastscale = False\n",
    "\n",
    "    def is_numword(x):\n",
    "        if is_number(x):\n",
    "            return True\n",
    "        if word in numwords:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def from_numword(x):\n",
    "        if is_number(x):\n",
    "            scale = 0\n",
    "            increment = int(x.replace(',', ''))\n",
    "            return scale, increment\n",
    "        return numwords[x]\n",
    "\n",
    "    for word in textnum.split():\n",
    "        if word in ordinal_words:\n",
    "            scale, increment = (1, ordinal_words[word])\n",
    "            current = current * scale + increment\n",
    "            if scale > 100:\n",
    "                result += current\n",
    "                current = 0\n",
    "            onnumber = True\n",
    "            lastunit = False\n",
    "            lastscale = False\n",
    "        else:\n",
    "            for ending, replacement in ordinal_endings:\n",
    "                if word.endswith(ending):\n",
    "                    word = \"%s%s\" % (word[:-len(ending)], replacement)\n",
    "\n",
    "            if (not is_numword(word)) or (word == 'and' and not lastscale):\n",
    "                if onnumber:\n",
    "                    # Flush the current number we are building\n",
    "                    curstring += repr(result + current) + \" \"\n",
    "                curstring += word + \" \"\n",
    "                result = current = 0\n",
    "                onnumber = False\n",
    "                lastunit = False\n",
    "                lastscale = False\n",
    "            else:\n",
    "                scale, increment = from_numword(word)\n",
    "                onnumber = True\n",
    "\n",
    "                if lastunit and (word not in scales):                                                                                                                                                                                                                                         \n",
    "                    # Assume this is part of a string of individual numbers to                                                                                                                                                                                                                \n",
    "                    # be flushed, such as a zipcode \"one two three four five\"                                                                                                                                                                                                                 \n",
    "                    curstring += repr(result + current)                                                                                                                                                                                                                                       \n",
    "                    result = current = 0                                                                                                                                                                                                                                                      \n",
    "\n",
    "                if scale > 1:                                                                                                                                                                                                                                                                 \n",
    "                    current = max(1, current)                                                                                                                                                                                                                                                 \n",
    "\n",
    "                current = current * scale + increment                                                                                                                                                                                                                                         \n",
    "                if scale > 100:                                                                                                                                                                                                                                                               \n",
    "                    result += current                                                                                                                                                                                                                                                         \n",
    "                    current = 0                                                                                                                                                                                                                                                               \n",
    "\n",
    "                lastscale = False                                                                                                                                                                                                              \n",
    "                lastunit = False                                                                                                                                                \n",
    "                if word in scales:                                                                                                                                                                                                             \n",
    "                    lastscale = True                                                                                                                                                                                                         \n",
    "                elif word in units:                                                                                                                                                                                                             \n",
    "                    lastunit = True\n",
    "\n",
    "    if onnumber:\n",
    "        curstring += repr(result + current)\n",
    "\n",
    "    return curstring\n",
    " \n",
    "Complete_Sample_2 = [[text2int(sent[0])] for sent in Complete_Sample]   \n",
    "\n",
    "#Complete_Sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for cnt in np.arange(0,len(Complete_Sample_2)):\n",
    "    Complete_Sample_2[cnt][0] = Complete_Sample_2[cnt][0].replace('as soon as possible','today')\n",
    "    Complete_Sample_2[cnt][0] = Complete_Sample_2[cnt][0].replace('asap','today')\n",
    "    Complete_Sample_2[cnt][0] = Complete_Sample_2[cnt][0].replace('ASAP','today')\n",
    "    \n",
    "#Complete_Sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Good Morning '],\n",
       " ['How can i help you? '],\n",
       " ['I would like to see if I can travel from Seattle to Chicago tomorrow '],\n",
       " ['When would you like to come back '],\n",
       " ['In 5 days, but I am flexible on the day '],\n",
       " ['What is your budget '],\n",
       " ['Less than 1500 dollars '],\n",
       " ['Great, Here are some options: ']]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for cnt in np.arange(0,len(Complete_Sample_2)):\n",
    "    Complete_Sample_2[cnt][0] = Complete_Sample_2[cnt][0].replace('$','dollars ')\n",
    "\n",
    "Complete_Sample_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good Morning']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "token_sent = []\n",
    "\n",
    "for response in Complete_Sample_2:\n",
    "    token_sent.append(sent_tokenize(response[0]))\n",
    "\n",
    "print(token_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Good', 'Morning'], ['How', 'can', 'i', 'help', 'you', '?'], ['I', 'would', 'like', 'to', 'see', 'if', 'I', 'can', 'travel', 'from', 'Seattle', 'to', 'Chicago', 'tomorrow'], ['When', 'would', 'you', 'like', 'to', 'come', 'back'], ['In', '5', 'days', ',', 'but', 'I', 'am', 'flexible', 'on', 'the', 'day'], ['What', 'is', 'your', 'budget'], ['Less', 'than', '1500', 'dollars'], ['Great', ',', 'Here', 'are', 'some', 'options', ':']]\n"
     ]
    }
   ],
   "source": [
    "token_word = []\n",
    "from nltk.tokenize import word_tokenize\n",
    "for word in token_sent:\n",
    "    token_word.append(word_tokenize(str(word[0])))\n",
    "\n",
    "print(token_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'morning', 'help', 'would', 'like', 'see', 'travel', 'seattle', 'chicago', 'tomorrow', 'would', 'like', 'come', 'back', '5', 'days', 'flexible', 'day', 'budget', 'less', '1500', 'dollars', 'great', 'options']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "no_punct = [] \n",
    "            \n",
    "for t in token_word:\n",
    "    for ts in t:\n",
    "        if ts not in string.punctuation:\n",
    "            no_punct.append(ts)\n",
    "\n",
    "data_lower = [w.lower() for w in no_punct]\n",
    "\n",
    "filtered_sent=[]\n",
    "\n",
    "for w in data_lower:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmer and Lemmatizer\n",
    "\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from nltk.stem import LancasterStemmer\n",
    "\n",
    "#ps= PorterStemmer()\n",
    "#ls = LancasterStemmer()\n",
    "\n",
    "#psstemmed_words=[]\n",
    "#lsstemmed_words = []\n",
    "\n",
    "#for w in filtered_sent:    \n",
    "    #psstemmed_words.append(ps.stem(w))\n",
    "    #lsstemmed_words.append(ls.stem(w))\n",
    "    #print(\"Words \"+w+ \"    PS :\"+ps.stem(w) + \"     LS : \"+ls.stem(w))\n",
    "\n",
    "    \n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "#lem = WordNetLemmatizer()\n",
    "#for word in filtered_sent:\n",
    "#    print (\"{0:20}{1:20}\".format(word,lem.lemmatize(word, pos =\"v\")))\n",
    "\n",
    "#today = date.today()\n",
    "#d1 = today.strftime(\"%B %d\")\n",
    "\n",
    "#print(str(d1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Massaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning / Massaging\n",
    "\n",
    "# Places -> Fine\n",
    "\n",
    "# Time and Dates\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "tomorrow = date.today() + datetime.timedelta(days=1)\n",
    "\n",
    "filtered_sent_2 = filtered_sent.copy()\n",
    "cnt = 0 \n",
    "\n",
    "for word in filtered_sent:\n",
    "    if word[-2:] == 'th' or word[-2:] == 'st' or word[-2:] == 'rd' or word[-2:] == 'nd':\n",
    "        try:\n",
    "            int(word[0])\n",
    "            print(word[:-2])\n",
    "            filtered_sent_2[cnt] = word[:-2]\n",
    "\n",
    "        except:\n",
    "            cnt2=1\n",
    "            \n",
    "    if word == 'tomorrow':        \n",
    "        filtered_sent_2[cnt] =  tomorrow.strftime(\"%d-%m-%Y\")\n",
    "        #print(d2)\n",
    "        \n",
    "    if word == 'today' or word == 'now':        \n",
    "        filtered_sent_2[cnt] = today.strftime(\"%d-%m-%Y\")       \n",
    "        #print(d1)    \n",
    "        \n",
    "    cnt = cnt +1\n",
    "\n",
    "#filtered_sent_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dl_pos =  nltk.pos_tag(data_lower)\n",
    "#dl_pos = default_tagger.tag(data_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.pos_tag(data_lower)\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "#t2.tag(filtered_sent_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsers\n",
    "\n",
    "https://www.nltk.org/api/nltk.parse.html\n",
    "\n",
    "Run specific parsers according to each type of information we want to extract?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle\n",
      "chicago\n"
     ]
    }
   ],
   "source": [
    "### Locations\n",
    "#https://www.geeksforgeeks.org/nlp-location-tags-extraction/\n",
    "import numpy as np\n",
    "from nltk.chunk import ChunkParserI \n",
    "from nltk.chunk.util import conlltags2tree \n",
    "from nltk.corpus import gazetteers \n",
    "  \n",
    "#sent_pos = nltk.pos_tag(data_lower)    \n",
    "words_tagged = t2.tag(filtered_sent_2)\n",
    "place_lower = [w.lower() for w in gazetteers.words()]\n",
    "\n",
    "loc_tag = words_tagged\n",
    "\n",
    "cnt=0\n",
    "for cnt in np.arange(1,len(words_tagged)-1):\n",
    "    if words_tagged[cnt][0] in place_lower:\n",
    "        if words_tagged[cnt][1] == 'NN':\n",
    "            print(words_tagged[cnt][0])\n",
    "            loc_tag[cnt] = (words_tagged[cnt][0],'LOCATION') \n",
    "            \n",
    "    link_place = words_tagged[cnt][0] + ' ' + words_tagged[cnt+1][0]        \n",
    "    if link_place in place_lower:      \n",
    "        if words_tagged[cnt][1] in ['JJ','NN'] and words_tagged[cnt+1][1] == 'NN':\n",
    "            print(link_place)\n",
    "            loc_tag[cnt] = (link_place,'LOCATION') \n",
    "            \n",
    "    cnt=cnt+1\n",
    "\n",
    "#print(loc_tag)\n",
    "#'/' in '25/03/2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time and Dates and Money\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tag = loc_tag.copy()\n",
    "\n",
    "month_lower = ['january','february','march','april','may','june',\n",
    "              'july','august','september','october','november','december',\n",
    "              'jan','feb','mar','apr','may','jun',\n",
    "              'jul','aug','sep','oct','nov','dec',\n",
    "               'day','days','week','weeks','months']\n",
    "\n",
    "cnt=0\n",
    "for cnt in np.arange(1,len(loc_tag)-1):\n",
    "    \n",
    "    if '-' in loc_tag[cnt][0]:\n",
    "                time_tag[cnt] = (loc_tag[cnt][0],'DATETIME') \n",
    "    \n",
    "    if loc_tag[cnt][0] in month_lower:\n",
    "        if loc_tag[cnt][1] == 'NN' or loc_tag[cnt][1] == 'NNS':\n",
    "                   \n",
    "            date_p2 = loc_tag[cnt-1][0]        \n",
    "            try:\n",
    "                int(date_p2)\n",
    "                time_tag[cnt] = (loc_tag[cnt][0] + ' ' + date_p2 ,'DATETIME') \n",
    "                #time_tag[cnt-1] = ('','NN') \n",
    "            except:\n",
    "                cnt2=1    \n",
    "                \n",
    "            date_p1 = loc_tag[cnt+1][0]        \n",
    "            try:\n",
    "                int(date_p1)\n",
    "                time_tag[cnt] = (loc_tag[cnt][0] + ' ' + date_p1,'DATETIME') \n",
    "                #time_tag[cnt+1] = ('','NN') \n",
    "            except:\n",
    "                cnt2=1\n",
    "    cnt=cnt+1\n",
    "\n",
    "#print(loc_tag)\n",
    "\n",
    "grammar = 'NumPhrase: {<CD><CD|NNS|JJ>}'\n",
    "t_parser = nltk.RegexpParser(grammar)\n",
    "\n",
    "final_tree = t_parser.parse(time_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  good/JJ\n",
      "  morning/NN\n",
      "  help/VB\n",
      "  would/MD\n",
      "  like/VB\n",
      "  see/VB\n",
      "  travel/NN\n",
      "  seattle/LOCATION\n",
      "  chicago/LOCATION\n",
      "  26-03-2020/DATETIME\n",
      "  would/MD\n",
      "  like/VB\n",
      "  come/VB\n",
      "  back/RB\n",
      "  5/CD\n",
      "  days 5/DATETIME\n",
      "  flexible/JJ\n",
      "  day/NN\n",
      "  budget/NN\n",
      "  less/QL\n",
      "  (NumPhrase 1500/CD dollars/NNS)\n",
      "  great/JJ\n",
      "  options/NN)\n",
      "[('good', 'JJ', 'O'), ('morning', 'NN', 'O'), ('help', 'VB', 'O'), ('would', 'MD', 'O'), ('like', 'VB', 'O'), ('see', 'VB', 'O'), ('travel', 'NN', 'O'), ('seattle', 'LOCATION', 'O'), ('chicago', 'LOCATION', 'O'), ('26-03-2020', 'DATETIME', 'O'), ('would', 'MD', 'O'), ('like', 'VB', 'O'), ('come', 'VB', 'O'), ('back', 'RB', 'O'), ('5', 'CD', 'O'), ('days 5', 'DATETIME', 'O'), ('flexible', 'JJ', 'O'), ('day', 'NN', 'O'), ('budget', 'NN', 'O'), ('less', 'QL', 'O'), ('1500', 'CD', 'B-NumPhrase'), ('dollars', 'NNS', 'I-NumPhrase'), ('great', 'JJ', 'O'), ('options', 'NN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import conlltags2tree, tree2conlltags\n",
    "\n",
    "print(final_tree)\n",
    "\n",
    "final_tags  = tree2conlltags(final_tree)\n",
    "print(final_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['seattle'], ['chicago']] [['26-03-2020'], ['days 5']] [['1500']]\n"
     ]
    }
   ],
   "source": [
    "#Find all important tags\n",
    "Locations = [[tag[0]] for tag in final_tags if tag[1] == 'LOCATION']  \n",
    "Locations\n",
    "\n",
    "Dates = [[tag[0]] for tag in final_tags if tag[1] == 'DATETIME']  \n",
    "Dates\n",
    "\n",
    "Money = [[tag[0]] for tag in final_tags if tag[2] == 'B-NumPhrase']  \n",
    "Money\n",
    "\n",
    "print(Locations,Dates,Money)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all dates into standard format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2020, 3, 26, 0, 0), datetime.date(2020, 3, 30)]\n"
     ]
    }
   ],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "date_clean = []\n",
    "\n",
    "cnt=0\n",
    "for d in Dates:\n",
    "    \n",
    "    if 'day' in d[0]:\n",
    "        tmp = d[0].split(' ')\n",
    "        d_plus = int(tmp[1])\n",
    "        date_clean.append(date.today() + datetime.timedelta(days=d_plus))\n",
    "    \n",
    "    elif 'week' in d[0]:\n",
    "        tmp = d[0].split(' ')\n",
    "        d_plus = int(tmp[1])\n",
    "        date_clean.append(date.today() + datetime.timedelta(days=d_plus*7))\n",
    "        \n",
    "    elif 'month' in d[0]:\n",
    "        tmp = d[0].split(' ')\n",
    "        d_plus = int(tmp[1])\n",
    "        date_clean.append(date.today() + datetime.timedelta(days=d_plus*30))\n",
    "        \n",
    "    else:\n",
    "        date_clean.append(parser.parse(d[0]))    \n",
    "    \n",
    "print(date_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK NER\n",
    "Chunking?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  good/JJ\n",
      "  morning/NN\n",
      "  help/VB\n",
      "  would/MD\n",
      "  like/VB\n",
      "  see/VB\n",
      "  travel/NN\n",
      "  seattle/LOCATION\n",
      "  chicago/LOCATION\n",
      "  26-03-2020/NN\n",
      "  would/MD\n",
      "  like/VB\n",
      "  come/VB\n",
      "  back/RB\n",
      "  5/CD\n",
      "  days/NNS\n",
      "  flexible/JJ\n",
      "  day/NN\n",
      "  budget/NN\n",
      "  less/QL\n",
      "  1500/CD\n",
      "  dollars/NNS\n",
      "  great/JJ\n",
      "  options/NN)\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from nltk import ne_chunk, pos_tag\n",
    "chunked = ne_chunk(loc_tag)\n",
    "\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Categorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic Engine to parse NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no second loaction, ask for start location\n",
    "\n",
    "# If no second date, assume one way\n",
    "\n",
    "# If no money, assume cheapest\n",
    "\n",
    "# If no dates, ask for dates, if not look at today/oneway\n",
    "\n",
    "# If more than 3 locations -> Multicity\n",
    "\n",
    "# Chatterbot\n",
    "# https://chatterbot.readthedocs.io/en/stable/training.html\n",
    "\n",
    "# Start Presentation Tomorrow\n",
    "    # Intro\n",
    "    # Methodology / Challenges\n",
    "    # Tech Stack / Processs\n",
    "    # Demo of Tagging\n",
    "    # Results/Next Steps\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
